library(tidyverse)
library(purrr)
str(iris)
#  We can set a seed for our random number generator to assure that
#  we will all have the same results.
set.seed(20220419)
#  Generate a list of row numbers (observation numbers) that we'll use
#  from our data set as the training observations.  Remember, we want
#  this to account for roughly 80% of our data.
sample_rows <- sample(1:nrow(iris), ceiling(.80 * nrow(iris)), replace = FALSE)
sample_rows
#  Remove any observations with NA.  Note, if you install tidyr, you can
#  use the drop_na() function such as iris <- iris %>% drop_na()
iris <- na.omit(iris)
# Checking for outliers
ggplot(iris) + geom_boxplot(aes(x = Sepal.Length))
ggplot(iris) + geom_boxplot(aes(x = Sepal.Width))
ggplot(iris) + geom_boxplot(aes(x = Petal.Length))
ggplot(iris) + geom_boxplot(aes(x = Petal.Width))
# Looks like there are outliers in Sepal.Width. We will remove them.
# Solid dots are mild outliers, which means there are more than 1.5 IQR's from Q3
(SWquartiles <- quantile(iris$Sepal.Width))
Q3 <- SWquartiles[[4]]
Q1 <- SWquartiles[[2]]
swIQR <- SWquartiles[[4]] - SWquartiles[[2]] # Q3 - Q1
iris <- iris %>% filter(Sepal.Width >= Q1 - 1.5 * swIQR & Sepal.Width <= Q3 + 1.5 * swIQR)
# Since we removed some values from iris, we need to recalculate our sample rows
sample_rows <- sample(1:nrow(iris), ceiling(.80 * nrow(iris)), replace = FALSE)
#  Now let's create a z-score function again, that takes
calc_z_score <- function(x) {
(x - mean(x)) / sd(x)
}
#  And now we can apply this to all but the last (fifth) column of iris
#  and call it n_iris
iris[1:4] <- iris %>% select(1:4) %>% lapply(calc_z_score)
head(iris)
#  And check that our mean is zero and standard deviation is one for each
lapply(iris[1:4], mean)
lapply(iris[1:4], sd)
#  Define iris_train and iris_test.  Can you do this using filter?
# We had sample_rows which lists the rows to keep for the training set.
iris_train <- iris[sample_rows,]
iris_test <- iris[-sample_rows,]
#  What if you wanted to use the dfname[ , ] notation?
library(MASS)
#  Now get the model.  We'll use the formula input option, i.e. we'll
#  indicate ResponseVar ~ PredVar1 + PredVar2 + .., or more simply
#  ResponseVar ~ . where the ~. means "all other variables"
model <- lda(formula = Species ~ . , data = iris_train)
model
#  See how our model is at determining the correct value for our predictions.
predicted <- predict(model, iris_test[-5])
predicted
#  And we can get an idea of what proportion our model correctly classified
mean(predicted$class == iris_test$Species)
#  Merge the data sets (as columns) from the training set and
#  and the LD1 and LD2 values associated with these from our model
lda_plot <- cbind(iris_train, predict(model)$x)
ggplot(lda_plot, aes(LD1, LD2)) +
geom_point(aes(color = Species))
